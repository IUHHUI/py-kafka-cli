#!/usr/bin/env python
"""
Kafka CLI - ç»Ÿä¸€çš„ Kafka å‘½ä»¤è¡Œå·¥å…·
"""
import argparse
import sys
import json
import base64
from kafka.admin import KafkaAdminClient
from kafka import KafkaConsumer, KafkaProducer
from kafka.structs import TopicPartition
from config import KafkaConfig


def cmd_list(args):
    """åˆ—å‡ºæ‰€æœ‰ Topics"""
    config = KafkaConfig.from_cli_args(
        bootstrap_servers=args.bootstrap_servers,
        client_id=args.client_id or 'kafka-cli-list'
    )

    try:
        admin_client = KafkaAdminClient(
            bootstrap_servers=config.bootstrap_servers,
            client_id=config.client_id
        )

        topic_list = admin_client.list_topics()

        print("Kafka é›†ç¾¤ä¸­çš„ Topics:")
        for topic in sorted(topic_list):
            print(f" - {topic}")

    except Exception as e:
        print(f"âŒ è¿æ¥æˆ–è·å– topic åˆ—è¡¨å¤±è´¥: {e}")
        sys.exit(1)


def cmd_cluster_id(args):
    """æ˜¾ç¤ºé›†ç¾¤ ID"""
    config = KafkaConfig.from_cli_args(
        bootstrap_servers=args.bootstrap_servers,
        client_id=args.client_id or 'kafka-cli-cluster'
    )

    try:
        admin_client = KafkaAdminClient(
            bootstrap_servers=config.bootstrap_servers,
            client_id=config.client_id
        )
        print("Kafka é›†ç¾¤ä¸­çš„ CLUSTER_ID:", admin_client._client.cluster.cluster_id)
    except Exception as e:
        print(f"âŒ è¿æ¥æˆ–è·å– CLUSTER_ID å¤±è´¥: {e}")
        sys.exit(1)


def cmd_show(args):
    """æ˜¾ç¤º Topic è¯¦ç»†ä¿¡æ¯"""
    config = KafkaConfig.from_cli_args(
        bootstrap_servers=args.bootstrap_servers,
        topic_name=args.topic,
        client_id=args.client_id or 'kafka-cli-show'
    )

    try:
        admin_client = KafkaAdminClient(
            bootstrap_servers=config.bootstrap_servers,
            client_id=config.client_id
        )

        cluster_id = getattr(admin_client._client.cluster, 'cluster_id', 'Unknown')
        print(f"âœ… é›†ç¾¤ ID: {cluster_id}\n")

        topic_metadata = admin_client.describe_topics([config.topic_name])

        if not topic_metadata:
            print(f"âŒ æ— æ³•è·å– topic '{config.topic_name}' çš„å…ƒæ•°æ®ï¼ˆå¯èƒ½ä¸å­˜åœ¨æˆ–æ— æƒé™ï¼‰")
            sys.exit(1)

        topic_info = topic_metadata[0]
        if topic_info.get("error_code") != 0:
            error_msg = topic_info.get("error_message", "Unknown error")
            print(f"âŒ è·å– topic å¤±è´¥: {error_msg}")
            sys.exit(1)

        print(f"ğŸ“Š Topic: '{topic_info['topic']}'")
        print(f"åˆ†åŒºæ€»æ•°: {len(topic_info['partitions'])}\n")

        for part in sorted(topic_info['partitions'], key=lambda x: x['partition']):
            partition_id = part['partition']
            leader = part['leader']
            replicas = part['replicas']
            isr = part['isr']

            print(f"åˆ†åŒº {partition_id}:")
            print(f"  Leader Broker: {leader}")
            print(f"  å‰¯æœ¬ (Replicas): {replicas}")
            print(f"  åŒæ­¥å‰¯æœ¬ (ISR):   {isr}")
            print("-" * 40)

        print("\nğŸ“¡ Broker åˆ—è¡¨:")
        cluster = admin_client._client.cluster
        broker_ids = cluster.brokers()

        if hasattr(cluster, '_brokers'):
            broker_metadata_dict = cluster._brokers
            for bid in sorted(broker_ids):
                bm = broker_metadata_dict.get(bid)
                if bm:
                    print(f"  Broker {bid}: {bm.host}:{bm.port}")
                else:
                    print(f"  Broker {bid}: <metadata not available>")
        else:
            for bid in sorted(broker_ids):
                print(f"  Broker {bid}: <host info unavailable>")

    except Exception as e:
        print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")
        sys.exit(1)
    finally:
        admin_client.close()


def cmd_count(args):
    """ç»Ÿè®¡æ¶ˆæ¯æ•°é‡"""
    config = KafkaConfig.from_cli_args(
        bootstrap_servers=args.bootstrap_servers,
        topic_name=args.topic,
        client_id=args.client_id or 'kafka-cli-count',
        timeout_ms=args.timeout
    )

    try:
        consumer = KafkaConsumer(
            bootstrap_servers=config.bootstrap_servers,
            request_timeout_ms=config.timeout_ms,
            client_id=config.client_id
        )

        partitions = consumer.partitions_for_topic(config.topic_name)
        if partitions is None:
            print(f"âŒ Topic '{config.topic_name}' ä¸å­˜åœ¨æˆ–æ— æ³•è®¿é—®")
            sys.exit(1)

        total_messages = 0
        print(f"æ­£åœ¨ç»Ÿè®¡ topic '{config.topic_name}' çš„æ¶ˆæ¯æ•°é‡...")

        for partition_id in partitions:
            tp = TopicPartition(config.topic_name, partition_id)

            beginning_offsets = consumer.beginning_offsets([tp])
            earliest = beginning_offsets[tp]

            end_offsets = consumer.end_offsets([tp])
            latest = end_offsets[tp]

            partition_count = latest - earliest
            total_messages += partition_count

            print(f"  åˆ†åŒº {partition_id}: {partition_count} æ¡æ¶ˆæ¯ (offset {earliest} â†’ {latest})")

        print(f"\nâœ… æ€»æ¶ˆæ¯æ•°ï¼ˆä¼°ç®—ï¼‰: {total_messages}")

    except Exception as e:
        print(f"âŒ ç»Ÿè®¡å¤±è´¥: {e}")
        sys.exit(1)
    finally:
        consumer.close()


def serialize_message(msg):
    """å°† Kafka æ¶ˆæ¯è½¬ä¸ºå¯ JSON åºåˆ—åŒ–çš„å­—å…¸"""
    return {
        'partition': msg.partition,
        'offset': msg.offset,
        'key': base64.b64encode(msg.key).decode('utf-8') if msg.key else None,
        'value': base64.b64encode(msg.value).decode('utf-8') if msg.value else None,
        'headers': [(k, base64.b64encode(v).decode('utf-8')) for k, v in msg.headers] if msg.headers else [],
        'timestamp': msg.timestamp
    }


def cmd_export(args):
    """å¯¼å‡ºæ¶ˆæ¯"""
    config = KafkaConfig.from_cli_args(
        bootstrap_servers=args.bootstrap_servers,
        topic_name=args.topic,
        client_id=args.client_id or 'kafka-cli-export',
        output_file=args.output_file,
        timeout_ms=args.timeout
    )

    output_file = config.output_file
    if not output_file:
        output_file = f'{config.topic_name}.jsonl'

    consumer = KafkaConsumer(
        bootstrap_servers=config.bootstrap_servers,
        auto_offset_reset='earliest',
        enable_auto_commit=False,
        consumer_timeout_ms=config.timeout_ms,
        client_id=config.client_id
    )

    try:
        partitions = consumer.partitions_for_topic(config.topic_name)
        if not partitions:
            raise RuntimeError(f"Topic {config.topic_name} not found")

        consumer.assign([TopicPartition(config.topic_name, p) for p in partitions])
        print(f"å¼€å§‹å¯¼å‡º topic '{config.topic_name}' å…± {len(partitions)} ä¸ªåˆ†åŒº...")

        count = 0
        with open(output_file, 'w', encoding='utf-8') as f:
            for msg in consumer:
                record = serialize_message(msg)
                f.write(json.dumps(record, ensure_ascii=False) + '\n')
                count += 1
                if count % 1000 == 0:
                    print(f"å·²å¯¼å‡º {count} æ¡æ¶ˆæ¯...")

        print(f"âœ… å¯¼å‡ºå®Œæˆï¼å…± {count} æ¡æ¶ˆæ¯ï¼Œä¿å­˜åˆ° {output_file}")
    except Exception as e:
        print(f"âŒ å¯¼å‡ºå¤±è´¥: {e}")
        sys.exit(1)
    finally:
        consumer.close()


def deserialize_record(record):
    """ä» JSON è®°å½•è¿˜åŸä¸ºåŸå§‹å­—æ®µ"""
    key = base64.b64decode(record['key']) if record['key'] else None
    value = base64.b64decode(record['value']) if record['value'] else None
    headers = [(k, base64.b64decode(v)) for k, v in record['headers']] if record['headers'] else None
    return {
        'key': key,
        'value': value,
        'headers': headers,
        'timestamp_ms': record['timestamp'] if record['timestamp'] > 0 else None
    }


def cmd_import(args):
    """å¯¼å…¥æ¶ˆæ¯"""
    config = KafkaConfig.from_cli_args(
        bootstrap_servers=args.bootstrap_servers,
        topic_name=args.topic,
        client_id=args.client_id or 'kafka-cli-import',
        input_file=args.input_file
    )

    input_file = config.input_file
    if not input_file:
        input_file = f'{config.topic_name}.jsonl'

    producer = KafkaProducer(
        bootstrap_servers=config.bootstrap_servers,
        client_id=config.client_id,
        acks=args.acks if args.acks is not None else config.acks,
        retries=args.retries if args.retries is not None else config.retries
    )

    count = 0
    try:
        with open(input_file, 'r', encoding='utf-8') as f:
            for line in f:
                if not line.strip():
                    continue
                record = json.loads(line)
                msg = deserialize_record(record)

                producer.send(
                    topic=config.topic_name,
                    key=msg['key'],
                    value=msg['value'],
                    headers=msg['headers'],
                    timestamp_ms=msg['timestamp_ms']
                )
                count += 1
                if count % 1000 == 0:
                    print(f"å·²å¯¼å…¥ {count} æ¡æ¶ˆæ¯...")

        producer.flush()
        print(f"âœ… å¯¼å…¥å®Œæˆï¼å…± {count} æ¡æ¶ˆæ¯åˆ° topic '{config.topic_name}'")
    except Exception as e:
        print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
        sys.exit(1)
    finally:
        producer.close()


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='Kafka CLI - ç»Ÿä¸€çš„ Kafka å‘½ä»¤è¡Œå·¥å…·',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  %(prog)s list -b localhost:9092
  %(prog)s show -t my-topic
  %(prog)s count -t my-topic
  %(prog)s export -t my-topic -o backup.jsonl
  %(prog)s import -t my-topic -i backup.jsonl

ç¯å¢ƒå˜é‡:
  KAFKA_BOOTSTRAP_SERVERS  é»˜è®¤ broker åœ°å€
  KAFKA_TOPIC              é»˜è®¤ topic åç§°
  KAFKA_CLIENT_ID          é»˜è®¤å®¢æˆ·ç«¯ ID
        """
    )

    # å…¨å±€å‚æ•°
    parser.add_argument('-b', '--bootstrap-servers', help=f'Kafka broker åœ°å€ (é»˜è®¤: {KafkaConfig().bootstrap_servers})')
    parser.add_argument('--client-id', help='å®¢æˆ·ç«¯ ID')

    # å­å‘½ä»¤
    subparsers = parser.add_subparsers(dest='command', help='å¯ç”¨å‘½ä»¤')

    # list å‘½ä»¤
    parser_list = subparsers.add_parser('list', help='åˆ—å‡ºæ‰€æœ‰ Topics')

    # cluster-id å‘½ä»¤
    parser_cluster = subparsers.add_parser('cluster-id', help='æ˜¾ç¤ºé›†ç¾¤ ID')

    # show å‘½ä»¤
    parser_show = subparsers.add_parser('show', help='æ˜¾ç¤º Topic è¯¦ç»†ä¿¡æ¯')
    parser_show.add_argument('-t', '--topic', required=True, help='Topic åç§°')

    # count å‘½ä»¤
    parser_count = subparsers.add_parser('count', help='ç»Ÿè®¡æ¶ˆæ¯æ•°é‡')
    parser_count.add_argument('-t', '--topic', required=True, help='Topic åç§°')
    parser_count.add_argument('--timeout', type=int, help='è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰')

    # export å‘½ä»¤
    parser_export = subparsers.add_parser('export', help='å¯¼å‡ºæ¶ˆæ¯åˆ°æ–‡ä»¶')
    parser_export.add_argument('-t', '--topic', required=True, help='Topic åç§°')
    parser_export.add_argument('-o', '--output-file', help='è¾“å‡ºæ–‡ä»¶è·¯å¾„ (é»˜è®¤: {topic}.jsonl)')
    parser_export.add_argument('--timeout', type=int, help='Consumer è¶…æ—¶æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰')

    # import å‘½ä»¤
    parser_import = subparsers.add_parser('import', help='ä»æ–‡ä»¶å¯¼å…¥æ¶ˆæ¯')
    parser_import.add_argument('-t', '--topic', required=True, help='Topic åç§°')
    parser_import.add_argument('-i', '--input-file', help='è¾“å…¥æ–‡ä»¶è·¯å¾„ (é»˜è®¤: {topic}.jsonl)')
    parser_import.add_argument('--acks', type=int, help='Producer acks è®¾ç½®')
    parser_import.add_argument('--retries', type=int, help='é‡è¯•æ¬¡æ•°')

    args = parser.parse_args()

    if not args.command:
        parser.print_help()
        sys.exit(1)

    # æ‰§è¡Œå¯¹åº”çš„å‘½ä»¤
    commands = {
        'list': cmd_list,
        'cluster-id': cmd_cluster_id,
        'show': cmd_show,
        'count': cmd_count,
        'export': cmd_export,
        'import': cmd_import,
    }

    if args.command in commands:
        commands[args.command](args)
    else:
        parser.print_help()
        sys.exit(1)


if __name__ == '__main__':
    main()
